"""
æ–‡è„ˆã‚’kv_cacheã¨ã—ã¦æ¸¡ã—è³ªå•æ–‡ã«ç¶šãæ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹
æ–‡è„ˆ+è³ªå•æ–‡ã®çµåˆã—ãŸtextã€è³ªå•æ–‡ã®ã¿ã®textã§ã®ç”Ÿæˆã¨æ¯”è¼ƒã™ã‚‹

kv_cacheã¨ã—ã¦æ¸¡ã—ãŸå ´åˆã¨ã€çµåˆã—ã¦æ¸¡ã—ãŸå ´åˆã§ã‚‚çµæœã¯åŒã˜ã«ãªã‚‹ã¯ãš
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class Questions():
    def __init__(self, with_context = True, with_text = False):
        self.with_context = with_context
        self.with_text = with_text
        self._i = 0
        self.questions = [
            ["ã‚¸ãƒ§ãƒ³ã¯çŠ¬ã‚’é£¼ã£ã¦ã„ã‚‹ãŒã€çŒ«ã¯é£¼ã£ã¦ã„ãªã„ã€‚","ã‚¸ãƒ§ãƒ³ã¯ã©ã‚“ãªå‹•ç‰©ã‚’é£¼ã£ã¦ã„ã¾ã™ã‹ï¼Ÿ"],
            ["æ˜¨æ—¥ã¯å¤§é›¨ã ã£ãŸãŒã€ä»Šæ—¥ã¯æ™´ã‚Œã¦ã„ã‚‹ã€‚", "ä»Šæ—¥ã®å¤©æ°—ã¯ã©ã†ã§ã™ã‹ï¼Ÿ"],
            ["ã‚¢ãƒªã‚¹ã¯ãƒ•ãƒ©ãƒ³ã‚¹ã«ç•™å­¦ã—ã¦ã„ãŸã®ã§ã€ãƒ•ãƒ©ãƒ³ã‚¹èªãŒå ªèƒ½ã ã€‚","ã‚¢ãƒªã‚¹ã¯ä½•èªãŒå¾—æ„ã§ã™ã‹ï¼Ÿ"],
            ["æœ¬æ—¥ã®ä¼šè­°ã¯ãƒªãƒ¢ãƒ¼ãƒˆã§ã¯ãªãã€ã‚ªãƒ•ã‚£ã‚¹ã§è¡Œã‚ã‚Œã‚‹ã€‚","ä¼šè­°ã¯ã©ã“ã§é–‹ã‹ã‚Œã¾ã™ã‹ï¼Ÿ"],
            ["æ˜¨å¤œã®è©¦åˆã§ã€ãƒ¬ãƒƒãƒ‰ãƒãƒ¼ãƒ ãŒãƒ–ãƒ«ãƒ¼ãƒãƒ¼ãƒ ã«å‹ã£ãŸã€‚", "ã©ã¡ã‚‰ã®ãƒãƒ¼ãƒ ãŒå‹ã¡ã¾ã—ãŸã‹ï¼Ÿ"],
            ["ã“ã®ç”ºã§ã¯ã€å†¬ã«ãªã‚‹ã¨é›ªãŒã‚ˆãé™ã‚‹ã€‚","å†¬ã®ã“ã®ç”ºã®å¤©æ°—ã¯ã©ã‚“ãªæ„Ÿã˜ã§ã™ã‹ï¼Ÿ"],
            ["ãƒãƒªã‚¢ã¯ãƒ™ã‚¸ã‚¿ãƒªã‚¢ãƒ³ãªã®ã§ã€è‚‰æ–™ç†ã¯é£Ÿã¹ãªã„ã€‚","ãƒãƒªã‚¢ã¯ã‚¹ãƒ†ãƒ¼ã‚­ã‚’é£Ÿã¹ã¾ã™ã‹ï¼Ÿ"],
            ["æ–°ã—ã„æ”¯åº—ã¯æ±äº¬ã§ã¯ãªãå¤§é˜ªã«é–‹è¨­ã•ã‚Œã‚‹ã€‚","æ–°ã—ã„æ”¯åº—ã¯ã©ã“ã«é–‹è¨­ã•ã‚Œã¾ã™ã‹ï¼Ÿ"],
            ["å½¼ã®èª•ç”Ÿæ—¥ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã¯ã€æ¥é€±ã®é‡‘æ›œæ—¥ã«äºˆå®šã•ã‚Œã¦ã„ã‚‹ã€‚","èª•ç”Ÿæ—¥ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã¯ã„ã¤è¡Œã‚ã‚Œã¾ã™ã‹ï¼Ÿ"],
            ["ãƒ¬ãƒãƒ¼ãƒˆã®ç· åˆ‡ã¯æœˆæ›œæ—¥ã‹ã‚‰æ°´æ›œæ—¥ã«å¤‰æ›´ã•ã‚ŒãŸã€‚","ãƒ¬ãƒãƒ¼ãƒˆã®ç· åˆ‡ã¯ä½•æ›œæ—¥ã§ã™ã‹ï¼Ÿ"]
        ]

    def __iter__(self):
        return self
    
    def __next__(self):
        if self._i >= len(self.questions):
            raise StopIteration
        question = self.questions[self._i]
        self._i += 1
        if self.with_context:
            return [question[0], question[1]]
        elif self.with_text:
            return [question[0] + question[1]]
        else:
            return [question[1]]



def gen(model, tokenizer, context = None, prompt = None):
    max_new_tokens = 20
    if context:
        memory_text = context
        memory_inputs = tokenizer(memory_text, return_tensors="pt")
        # memory_inputsã‚’ãƒ¢ãƒ‡ãƒ«ã«é€šã—ã¦ã€past_key_valuesã ã‘å–å¾—
        with torch.no_grad():
            outputs = model(**memory_inputs, use_cache=True)
            memory_past_key_values = outputs.past_key_values  # ã“ã‚ŒãŒKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆList[Tuple(K, V)])

        # prompt_text = prompt
        # prompt_inputs = tokenizer(prompt_text, return_tensors="pt")
        messages = [
            [
                {
                    "role": "system",
                    "content":  "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
                },
                {
                    "role": "user",
                    "content":  prompt
                },
            ],
        ]
        prompt_inputs = tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
        )


        memory_length = memory_inputs.input_ids.shape[1]
        prompt_position_ids = torch.arange(memory_length, memory_length + prompt_inputs.input_ids.shape[1]).unsqueeze(0)

        memory_attention_mask = torch.ones((1, memory_length), dtype=torch.long)
        combined_attention_mask = torch.cat([memory_attention_mask, prompt_inputs.attention_mask], dim=1)

        # â‘¢ KVæ³¨å…¥ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œ
        # æ³¨ï¼šattention_maskã¯ã€prompt_inputsã«å¯¾å¿œã™ã‚‹ã‚‚ã®ã ã‘ã‚’ä½¿ã†
        with torch.no_grad():
            # æœ¬æ¥ãªã‚‰memory_textã®å¾Œã«promptã‚’é€£çµã—ã¦ä¸€ç·’ã«æ¨è«–ã™ã‚‹ãŒã€
            # ä»Šå›ã¯KVã ã‘å¼•ãç¶™ã„ã§promptã ã‘æ–°è¦ã«é£Ÿã‚ã›ã‚‹
            generated_outputs = model.generate(
                input_ids=prompt_inputs.input_ids,
                attention_mask=combined_attention_mask,
                use_cache=True,
                max_new_tokens=max_new_tokens,  # ğŸ”¥ ã“ã“ãŒè¿½åŠ ç‚¹
                do_sample=False,  # Greedy decodingï¼ˆç¢ºç‡æœ€å¤§ã‚’é¸æŠï¼‰
                position_ids=prompt_position_ids,
                past_key_values=memory_past_key_values,
            )
    else:
        # prompt_text = prompt
        # prompt_inputs = tokenizer(prompt_text, return_tensors="pt")
        messages = [
            [
                {
                    "role": "system",
                    "content":  "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
                },
                {
                    "role": "user",
                    "content":  prompt
                },
            ],
        ]
        prompt_inputs = tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
        )
        with torch.no_grad():
            generated_outputs = model.generate(
                input_ids=prompt_inputs.input_ids,
                attention_mask=prompt_inputs.attention_mask,
                use_cache=True,
                max_new_tokens=max_new_tokens,
                do_sample=False,
            )

    generated_text = tokenizer.decode(generated_outputs[0], skip_special_tokens=True)
    print("=== Generated Text ===")

    print(generated_text)

def main():
    model_name = "microsoft/Phi-4-mini-instruct"
    # model_name = "microsoft/phi-4"

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
    model.eval()

    ## æ–‡è„ˆkv_cache, è³ªå•æ–‡
    questions = Questions(with_context=True, with_text=False)
    for question in questions:
        context = question[0]
        prompt = question[1]
        print(f"Context: {context}")
        print(f"Prompt: {prompt}")
        gen(model, tokenizer, context=context, prompt=prompt)

    ## æ–‡è„ˆ+è³ªå•æ–‡ã®çµåˆ
    questions = Questions(with_context=False, with_text=True)
    for question in questions:
        prompt = question[0]
        print(f"Prompt: {prompt}")
        gen(model, tokenizer, context=None, prompt=prompt)

    ## è³ªå•æ–‡ã®ã¿
    questions = Questions(with_context=False)
    for question in questions:
        prompt = question[0]
        print(f"Prompt: {prompt}")
        gen(model, tokenizer, context=None, prompt=prompt)

if __name__ == "__main__":
    main()